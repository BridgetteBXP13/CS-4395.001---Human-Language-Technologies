{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd95ec68-5c69-47dc-8061-fb1447084793",
   "metadata": {},
   "source": [
    "# Exploring NLTK\n",
    "#### Bridgette Bryant\n",
    "###### CS 4395.001, Human Language Technologies, Assignment 2\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a26a1-36be-4df2-a0cb-5a87a1c6d67b",
   "metadata": {},
   "source": [
    "Imports the NLTK library and then imports nltk.book and prints some introductory examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5846b1b3-2319-484d-9c17-2d8da0b7d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da72b54c-fdb0-4fac-859b-3c2cde71897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fef97-f5a5-4c68-b8a0-961cce9c7425",
   "metadata": {},
   "source": [
    "Sets the contents of nltk.book to a Text object variable named text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408ac17-73c2-4186-9224-e79db25eebf4",
   "metadata": {},
   "source": [
    "This code section utilized the .tokens() method from the NLTK Text object. \n",
    "I learned that tokens is a required list in the Text Class given when creating a Text object. All text objects have a tokenized list already which you can get using the text.tokens command.\n",
    "I also learned that because it is already created, you can used the text.tokens command as a list utilizing indexing in orer to avoid doing uneccessary space with a variable (as shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18ec0d0-1a21-49ac-9dff-f6a63f1e0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "Moby\n",
      "Dick\n",
      "by\n",
      "Herman\n",
      "Melville\n",
      "1851\n",
      "]\n",
      "ETYMOLOGY\n",
      ".\n",
      "(\n",
      "Supplied\n",
      "by\n",
      "a\n",
      "Late\n",
      "Consumptive\n",
      "Usher\n",
      "to\n",
      "a\n",
      "Grammar\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20): print(text1.tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549db858-123d-4790-b36f-20ddfe598513",
   "metadata": {},
   "source": [
    "The command below utilized the concordance() method from the NLTK Text Object, it prints the concordance for text1 word 'sea', selecting only the first 5 lines, with 79 being the default line width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc709d1-29ea-411d-a27f-0ba24c36dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
     ]
    }
   ],
   "source": [
    "text1.concordance('sea', 79, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e57ea-c633-4781-af22-02532a8a9b55",
   "metadata": {},
   "source": [
    "The Text object count() method workds the same as the List object count() method. It will return the number of times an object appears in the list. I know this because the way the count function works in this Text API is by calling text.tokens function (which returns a list of already processed tokens) and using the List object count() function on that list. In other words, text1.count() is the same as text1.tokens.count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf10ee3c-eb84-460a-9208-6c2b044f9fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('sea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd89aac-bd5e-4ab9-b83b-b1e705cf38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.tokens.count('sea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49290da-d160-4eb7-aae4-47ac67304fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016e6dd4-61b2-46fc-8158-4395476d26a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.tokens.count('whale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47211d-88d2-4794-a963-272f3d709d68",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "##### My Choosen Raw Text for the next example:\n",
    "\"Techkla Minnau. (Pause) Teckla is one of my aides. Like so many the\n",
    "people we tell ourselves we're here to serve, Teckla lives in a \n",
    "district that rarely has electricity and running water as a result of\n",
    "the war. Her childen can now only bathe every two weeks, and they have no light in which to read or study at night. The Republic has always funded these basic services, but now, there are those who would divert the money to the war with no thought for what the people need to survive. If not for people like Teckla and her children, who are we fighting for? My people, your people, all of our people. This war is meant to save them from suffering, not increase it. I support our brave soldiers, whether they come from the clone factories or from any of the thousands of systems loyal to the Republic, but if we\n",
    "continue to impoverish our people, it is not on the battlefield where\n",
    "Dooku will defeat us, but in our own homes. Therefore, it is our duty\n",
    "and responsibility to preserve the lives of those around us by defeating this bill.\" - Senator Amidala, Padme\n",
    "(Star Wars: The Clone Wars - Season 3 Episode 11)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5e594-26ec-407f-bcf9-dfcd30b27616",
   "metadata": {},
   "source": [
    "The code below simply sets the raw_text variable to the choosen text\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1709fa98-64eb-44aa-9ece-b5f9be03727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Techkla Minnau. (Pause) Teckla is one of my aides. Like so many the people we tell ourselves we're here to serve, Teckla lives in a district that rarely has electricity and running water as a result of the war. Her childen can now only bathe every two weeks, and they have no light in which to read or study at night. The Republic has always funded these basic services, but now, there are those who would divert the money to the war with no thought for what the people need to survive. If not for people like Teckla and her children, who are we fighting for? My people, your people, all of our people. This war is meant to save them from suffering, not increase it. I support our brave soldiers, whether they come from the clone factories or from any of the thousands of systems loyal to the Republic, but if we continue to impoverish our people, it is not on the battlefield where Dooku will defeat us, but in our own homes. Therefore, it is our duty and responsibility to preserve the lives of those around us by defeating this bill.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff4580-389c-45c8-996e-2bf84fe075ef",
   "metadata": {},
   "source": [
    "Import the word tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b7d1a8-19b0-4efa-b32f-244c58021457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a4a50-5f33-461f-bccd-509299c6bbcd",
   "metadata": {},
   "source": [
    "The following code line simply tokenizes the raw_text by words using the NLTK word_tokenize and saves it into a variable named tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f1bb2f-ce39-42a8-8efc-634e300fd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc14927-b0c2-49ef-a51e-068d244bbf9c",
   "metadata": {},
   "source": [
    "The following code line prints the first 10 tokens from the code above, aka the first 10 tokenized words of raw_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b182d3-5c62-4843-8c06-3d44c38f5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Techkla', 'Minnau', '.', '(', 'Pause', ')', 'Teckla', 'is', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d1814-2bfc-4eea-9d3c-ad5e4eb078fd",
   "metadata": {},
   "source": [
    "Import the sentence tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36605e1b-c1bc-475d-a65c-1ab252eb8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed2cb9-2f86-4c39-9f77-16806175bfa1",
   "metadata": {},
   "source": [
    "The following code line simply tokenizes the raw_text by sentences using the NLTK sent_tokenize and prints the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3873c08-42f7-4ee6-a137-a142c7bb3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Techkla Minnau.', '(Pause) Teckla is one of my aides.', \"Like so many the people we tell ourselves we're here to serve, Teckla lives in a district that rarely has electricity and running water as a result of the war.\", 'Her childen can now only bathe every two weeks, and they have no light in which to read or study at night.', 'The Republic has always funded these basic services, but now, there are those who would divert the money to the war with no thought for what the people need to survive.', 'If not for people like Teckla and her children, who are we fighting for?', 'My people, your people, all of our people.', 'This war is meant to save them from suffering, not increase it.', 'I support our brave soldiers, whether they come from the clone factories or from any of the thousands of systems loyal to the Republic, but if we continue to impoverish our people, it is not on the battlefield where Dooku will defeat us, but in our own homes.', 'Therefore, it is our duty and responsibility to preserve the lives of those around us by defeating this bill.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bc1b5-0418-449e-a203-e320cff8b67e",
   "metadata": {},
   "source": [
    "Import the stemmers from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c6c91f-9f9b-46c3-aee9-a2714b4a2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8b26b-2ef7-443b-affe-60efa9b77175",
   "metadata": {},
   "source": [
    "Create a Porter Stemmer named stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c40694-0bcf-4149-8881-f1f4bf7319b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a67292-1c3b-4031-9256-215f4ded69ca",
   "metadata": {},
   "source": [
    "Stem all the tokens (the tokenized words from raw_text above) and save them into a variable named stemmed. Then print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51ee2be-ca4a-4beb-893e-7de7474c8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['techkla', 'minnau', '.', '(', 'paus', ')', 'teckla', 'is', 'one', 'of', 'my', 'aid', '.', 'like', 'so', 'mani', 'the', 'peopl', 'we', 'tell', 'ourselv', 'we', \"'re\", 'here', 'to', 'serv', ',', 'teckla', 'live', 'in', 'a', 'district', 'that', 'rare', 'ha', 'electr', 'and', 'run', 'water', 'as', 'a', 'result', 'of', 'the', 'war', '.', 'her', 'childen', 'can', 'now', 'onli', 'bath', 'everi', 'two', 'week', ',', 'and', 'they', 'have', 'no', 'light', 'in', 'which', 'to', 'read', 'or', 'studi', 'at', 'night', '.', 'the', 'republ', 'ha', 'alway', 'fund', 'these', 'basic', 'servic', ',', 'but', 'now', ',', 'there', 'are', 'those', 'who', 'would', 'divert', 'the', 'money', 'to', 'the', 'war', 'with', 'no', 'thought', 'for', 'what', 'the', 'peopl', 'need', 'to', 'surviv', '.', 'if', 'not', 'for', 'peopl', 'like', 'teckla', 'and', 'her', 'children', ',', 'who', 'are', 'we', 'fight', 'for', '?', 'my', 'peopl', ',', 'your', 'peopl', ',', 'all', 'of', 'our', 'peopl', '.', 'thi', 'war', 'is', 'meant', 'to', 'save', 'them', 'from', 'suffer', ',', 'not', 'increas', 'it', '.', 'i', 'support', 'our', 'brave', 'soldier', ',', 'whether', 'they', 'come', 'from', 'the', 'clone', 'factori', 'or', 'from', 'ani', 'of', 'the', 'thousand', 'of', 'system', 'loyal', 'to', 'the', 'republ', ',', 'but', 'if', 'we', 'continu', 'to', 'impoverish', 'our', 'peopl', ',', 'it', 'is', 'not', 'on', 'the', 'battlefield', 'where', 'dooku', 'will', 'defeat', 'us', ',', 'but', 'in', 'our', 'own', 'home', '.', 'therefor', ',', 'it', 'is', 'our', 'duti', 'and', 'respons', 'to', 'preserv', 'the', 'live', 'of', 'those', 'around', 'us', 'by', 'defeat', 'thi', 'bill', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(t) for t in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575baeb-3a7e-4f13-90b8-4a32856e434e",
   "metadata": {},
   "source": [
    "Create a Word Net Lemmatizer named wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423a6354-f807-4270-9a04-c38eb37e7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c9aa7-c93b-40dc-85d4-5bdba465fff6",
   "metadata": {},
   "source": [
    "Lemmatize all the tokens (the tokenized words from raw_text above) and save them into a variable named lemmatized. Then print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac620d9-e1e6-4337-97fb-8dcc1a383371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Techkla', 'Minnau', '.', '(', 'Pause', ')', 'Teckla', 'is', 'one', 'of', 'my', 'aide', '.', 'Like', 'so', 'many', 'the', 'people', 'we', 'tell', 'ourselves', 'we', \"'re\", 'here', 'to', 'serve', ',', 'Teckla', 'life', 'in', 'a', 'district', 'that', 'rarely', 'ha', 'electricity', 'and', 'running', 'water', 'a', 'a', 'result', 'of', 'the', 'war', '.', 'Her', 'childen', 'can', 'now', 'only', 'bathe', 'every', 'two', 'week', ',', 'and', 'they', 'have', 'no', 'light', 'in', 'which', 'to', 'read', 'or', 'study', 'at', 'night', '.', 'The', 'Republic', 'ha', 'always', 'funded', 'these', 'basic', 'service', ',', 'but', 'now', ',', 'there', 'are', 'those', 'who', 'would', 'divert', 'the', 'money', 'to', 'the', 'war', 'with', 'no', 'thought', 'for', 'what', 'the', 'people', 'need', 'to', 'survive', '.', 'If', 'not', 'for', 'people', 'like', 'Teckla', 'and', 'her', 'child', ',', 'who', 'are', 'we', 'fighting', 'for', '?', 'My', 'people', ',', 'your', 'people', ',', 'all', 'of', 'our', 'people', '.', 'This', 'war', 'is', 'meant', 'to', 'save', 'them', 'from', 'suffering', ',', 'not', 'increase', 'it', '.', 'I', 'support', 'our', 'brave', 'soldier', ',', 'whether', 'they', 'come', 'from', 'the', 'clone', 'factory', 'or', 'from', 'any', 'of', 'the', 'thousand', 'of', 'system', 'loyal', 'to', 'the', 'Republic', ',', 'but', 'if', 'we', 'continue', 'to', 'impoverish', 'our', 'people', ',', 'it', 'is', 'not', 'on', 'the', 'battlefield', 'where', 'Dooku', 'will', 'defeat', 'u', ',', 'but', 'in', 'our', 'own', 'home', '.', 'Therefore', ',', 'it', 'is', 'our', 'duty', 'and', 'responsibility', 'to', 'preserve', 'the', 'life', 'of', 'those', 'around', 'u', 'by', 'defeating', 'this', 'bill', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b365db9-1a46-4b57-bc8e-b90bb5019b53",
   "metadata": {},
   "source": [
    "Stems                            vs.                           Lemmas\n",
    "---------------------------------------------------------------------\n",
    "1. Stems keep 'as' as a word, but lemmas remove the 's' from as making it 'a'.\n",
    "2. Stems cut off the ends of words such as 'e', 'ed', and 'es', but lemmas keep endings of words but removes 's'.\n",
    "3. Stems changes 'y' to 'i', but lemmas keep 'y'.\n",
    "4. Stems remove 's' in pronouns such as 'his' to 'hi', lemmas keep the 's' in pronouns keeping it 'his'. \n",
    "5. Stems make everything lowercase, lemmas keeps the given uppercase letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6d17d-4b66-4fd2-b21a-75cde21a369b",
   "metadata": {},
   "source": [
    "# My opinion of NLTK\n",
    "NLTK has many functionalities for processing text data such as tokenizing, stemming, lemmatizing, translating, creating your own grammars, and much more. To tokenize data you can use many different tokenizers such as word_tokenize to tokenize by words, sent_tokenize to tokenize by sentences, or WhitespaceTokenizer to tokenize by whitespace. To stem/lemmatize there are also different flavors to choose from. These prebuilt modules ih NLTK are very robust and programmer-friendly to implement. This makes them high quality as they have precise error messages and most of them are very straight forward to use. For the more complex modules NLTK provides extensive documentation of all thier modules with examples. If none of those help somehow it is also a very popular resouce so there are even more examples/implementation online, making it unlikely that you will get stuck on trying to figure out how to use an NLTK module. I will likely use NLTK in future projects to tokenize, stem, and lemmatize data. I may also utilize classification, chunk, parsing, and tagging to analyze data. Possibly might use the probability and toolbox to analyze and predict text patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
